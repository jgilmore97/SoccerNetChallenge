{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyMh3kEkne24oECWLJLF6VSF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hgLIhy-9l4R6","executionInfo":{"status":"ok","timestamp":1733532742284,"user_tz":480,"elapsed":112360,"user":{"displayName":"Adithya Embar","userId":"03622903149941864887"}},"outputId":"470f7369-0a9a-4b7d-8f40-567bbaebec66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# install dependencies\n","!pip install torch torchvision captum SoccerNet timm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKM293wumRSl","executionInfo":{"status":"ok","timestamp":1733532749107,"user_tz":480,"elapsed":6827,"user":{"displayName":"Adithya Embar","userId":"03622903149941864887"}},"outputId":"3032a481-c2ba-4322-9624-22a59196b17d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Collecting captum\n","  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n","Collecting SoccerNet\n","  Downloading SoccerNet-0.1.61-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.6)\n","Collecting scikit-video (from SoccerNet)\n","  Downloading scikit_video-1.1.11-py2.py3-none-any.whl.metadata (1.1 kB)\n","Collecting google-measurement-protocol (from SoccerNet)\n","  Downloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl.metadata (845 bytes)\n","Collecting pycocoevalcap (from SoccerNet)\n","  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: huggingface-hub[cli] in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (0.26.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n","Requirement already satisfied: requests<3.0a0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from google-measurement-protocol->SoccerNet) (2.32.3)\n","Collecting prices>=1.0.0 (from google-measurement-protocol->SoccerNet)\n","  Downloading prices-1.1.1-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (24.2)\n","Collecting InquirerPy==0.3.4 (from huggingface-hub[cli]->SoccerNet)\n","  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n","Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet)\n","  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (3.0.48)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap->SoccerNet) (2.0.8)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video->SoccerNet) (1.13.1)\n","Requirement already satisfied: babel>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from prices>=1.0.0->google-measurement-protocol->SoccerNet) (2.16.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2024.8.30)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (0.2.13)\n","Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading SoccerNet-0.1.61-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl (5.9 kB)\n","Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading prices-1.1.1-py3-none-any.whl (9.5 kB)\n","Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n","Installing collected packages: prices, pfzy, scikit-video, InquirerPy, google-measurement-protocol, captum, pycocoevalcap, SoccerNet\n","Successfully installed InquirerPy-0.3.4 SoccerNet-0.1.61 captum-0.7.0 google-measurement-protocol-1.1.0 pfzy-0.3.4 prices-1.1.1 pycocoevalcap-1.2 scikit-video-1.1.11\n"]}]},{"cell_type":"code","source":["# clone the repository\n","!git clone https://github.com/SoccerNet/sn-spotting.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PaZRUP2qmR6s","executionInfo":{"status":"ok","timestamp":1733532767960,"user_tz":480,"elapsed":18856,"user":{"displayName":"Adithya Embar","userId":"03622903149941864887"}},"outputId":"5c7627a7-6fe0-4e84-8468-38bbc8256b06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'sn-spotting'...\n","remote: Enumerating objects: 439, done.\u001b[K\n","remote: Counting objects: 100% (129/129), done.\u001b[K\n","remote: Compressing objects: 100% (79/79), done.\u001b[K\n","remote: Total 439 (delta 82), reused 78 (delta 46), pack-reused 310 (from 1)\u001b[K\n","Receiving objects: 100% (439/439), 252.86 MiB | 17.23 MiB/s, done.\n","Resolving deltas: 100% (163/163), done.\n","Updating files: 100% (185/185), done.\n"]}]},{"cell_type":"code","source":["%pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"2hC8dNflmVju","executionInfo":{"status":"ok","timestamp":1733532767961,"user_tz":480,"elapsed":19,"user":{"displayName":"Adithya Embar","userId":"03622903149941864887"}},"outputId":"d06f1db1-bee7-4680-a147-fee95a3f83b8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# load the model checkpoint from google drive\n","!mkdir -p models/CALF_v2/\n","\n","!cp drive/MyDrive/files/soccernet/model.pth.tar models/CALF_v2/"],"metadata":{"id":"emfVm-HymYLL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Classes.py"],"metadata":{"id":"DL-vUNP4myam"}},{"cell_type":"code","source":["# classes.py\n","\n","import torch\n","\n","\n","# Event name to label index fororor SoccerNet-V2\n","EVENT_DICTIONARY_V2 = {\"Penalty\":0,\"Kick-off\":1,\"Goal\":2,\"Substitution\":3,\"Offside\":4,\"Shots on target\":5,\n","                                \"Shots off target\":6,\"Clearance\":7,\"Ball out of play\":8,\"Throw-in\":9,\"Foul\":10,\n","                                \"Indirect free-kick\":11,\"Direct free-kick\":12,\"Corner\":13,\"Yellow card\":14\n","                                ,\"Red card\":15,\"Yellow->red card\":16}\n","\n","INVERSE_EVENT_DICTIONARY_V2 = {0:\"Penalty\",1:\"Kick-off\",2:\"Goal\",3:\"Substitution\",4:\"Offside\",5:\"Shots on target\",\n","                                6:\"Shots off target\",7:\"Clearance\",8:\"Ball out of play\",9:\"Throw-in\",10:\"Foul\",\n","                                11:\"Indirect free-kick\",12:\"Direct free-kick\",13:\"Corner\",14:\"Yellow card\"\n","                                ,15:\"Red card\",16:\"Yellow->red card\"}\n","\n","# Values of the K parameters (in seconds) in the context-aware loss\n","K_V2 = torch.FloatTensor([[-100, -98, -20, -40, -96, -5, -8, -93, -99, -31, -75, -10, -97, -75, -20, -84, -18], [-50, -49, -10, -20, -48, -3, -4, -46, -50, -15, -37, -\n","                                                                                                                 5, -49, -38, -10, -42, -9], [50, 49, 60, 10, 48, 3, 4, 46, 50, 15, 37, 5, 49, 38, 10, 42, 9], [100, 98, 90, 20, 96, 5, 8, 93, 99, 31, 75, 10, 97, 75, 20, 84, 18]]).cuda()\n"],"metadata":{"id":"CqYX0qyKmaB9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing.py"],"metadata":{"id":"AxbBlkCSm3Wj"}},{"cell_type":"code","source":["# preprocessing.py\n","\n","\n","import numpy as np\n","import torch\n","\n","def rulesToCombineShifts(shift_from_last_event, shift_until_next_event, params):\n","\n","    s1  = shift_from_last_event\n","    s2  = shift_until_next_event\n","    K = params\n","\n","    if s1 < K[2]:\n","        value = s1\n","    elif s1 < K[3]:\n","        if s2 <= K[0]:\n","            value = s1\n","        else:\n","            if (s1-K[2])/(K[3]-K[2]) < (K[1]-s2)/(K[1]-K[0]):\n","                value = s1\n","            else:\n","                value = s2\n","    else:\n","        value = s2\n","\n","    return value\n","\n","def oneHotToShifts(onehot, params):\n","\n","\n","    nb_frames = onehot.shape[0]\n","    nb_actions = onehot.shape[1]\n","\n","    Shifts = np.empty(onehot.shape)\n","\n","    for i in range(nb_actions):\n","\n","        x = onehot[:,i]\n","        K = params[:,i]\n","        shifts = np.empty(nb_frames)\n","\n","        loc_events = np.where(x == 1)[0]\n","        nb_events = len(loc_events)\n","\n","        if nb_events == 0:\n","            shifts = np.full(nb_frames, K[0])\n","        elif nb_events == 1:\n","            shifts = np.arange(nb_frames) - loc_events\n","        else:\n","            loc_events = np.concatenate(([-K[3]],loc_events,[nb_frames-K[0]]))\n","            for j in range(nb_frames):\n","                shift_from_last_event = j - loc_events[np.where(j >= loc_events)[0][-1]]\n","                shift_until_next_event = j - loc_events[np.where(j < loc_events)[0][0]]\n","                shifts[j] = rulesToCombineShifts(shift_from_last_event, shift_until_next_event, K)\n","\n","        Shifts[:,i] = shifts\n","\n","    return Shifts\n","\n","import random\n","\n","\n","def getNegativeIndexes(labels, params, chunk_size):\n","\n","    zero_one_labels = np.zeros(labels.shape)\n","    for i in np.arange(labels.shape[1]):\n","        zero_one_labels[:,i] = 1-np.logical_or(np.where(labels[:,i] >= params[3,i], 1,0),np.where(labels[:,i] <= params[0,i], 1,0))\n","    zero_one = np.where(np.sum(zero_one_labels, axis=1)>0, 0, 1)\n","\n","    zero_one_pad = np.append(np.append([1-zero_one[0],], zero_one, axis=0), [1-zero_one[-1]], axis=0)\n","    zero_one_pad_shift = np.append(zero_one_pad[1:], zero_one_pad[-1])\n","\n","    zero_one_sub = zero_one_pad - zero_one_pad_shift\n","\n","    zero_to_one_index = np.where(zero_one_sub == -1)[0]\n","    one_to_zero_index = np.where(zero_one_sub == 1)[0]\n","\n","\n","    if zero_to_one_index[0] > one_to_zero_index[0]:\n","        one_to_zero_index = one_to_zero_index[1:]\n","    if zero_to_one_index.shape[0] > one_to_zero_index.shape[0]:\n","        zero_to_one_index = zero_to_one_index[:-1]\n","\n","    list_indexes = list()\n","\n","    for i,j in zip(zero_to_one_index, one_to_zero_index):\n","        if j-i >= chunk_size:\n","            list_indexes.append([i,j])\n","\n","    return list_indexes\n","\n","\n","def getChunks_anchors(labels, game_index, params, chunk_size=240, receptive_field=80):\n","\n","    # get indexes of labels\n","    indexes=list()\n","    for i in np.arange(labels.shape[1]):\n","        indexes.append(np.where(labels[:,i] == 0)[0].tolist())\n","\n","    # Positive chunks\n","    anchors = list()\n","\n","    class_counter = 0\n","    for event in indexes:\n","        for element in event:\n","            anchors.append([game_index,element,class_counter])\n","        class_counter += 1\n","\n","\n","    # Negative chunks\n","\n","    negative_indexes = getNegativeIndexes(labels, params, chunk_size)\n","\n","    for negative_index in negative_indexes:\n","        start = [negative_index[0], negative_index[1]]\n","        anchors.append([game_index,start,labels.shape[1]])\n","\n","\n","    return anchors\n","\n","def getTimestampTargets(labels, num_detections):\n","\n","    targets = np.zeros((labels.shape[0],num_detections,2+labels.shape[-1]), dtype='float')\n","\n","    for i in np.arange(labels.shape[0]):\n","\n","        time_indexes, class_values = np.where(labels[i]==0)\n","\n","        counter = 0\n","\n","        for time_index, class_value in zip(time_indexes, class_values):\n","\n","            # Confidence\n","            targets[i,counter,0] = 1.0\n","            # frame index normalized\n","            targets[i,counter,1] = time_index/(labels.shape[1])\n","            # The class one hot encoded\n","            targets[i,counter,2+class_value] = 1.0\n","            counter += 1\n","\n","            if counter >= num_detections:\n","                print(\"More timestamp than what was fixed... A lot happened in that chunk\")\n","                break\n","\n","    return targets\n","\n","\n","\n","# Function to transform the timestamps to vectors\n","def timestamps2long(output_spotting, video_size, chunk_size, receptive_field):\n","\n","    start = 0\n","    last = False\n","    receptive_field = receptive_field//2\n","\n","    timestamps_long = torch.zeros([video_size,output_spotting.size()[-1]-2], dtype = torch.float, device=output_spotting.device)-1\n","\n","\n","    for batch in np.arange(output_spotting.size()[0]):\n","\n","        tmp_timestamps = torch.zeros([chunk_size,output_spotting.size()[-1]-2], dtype = torch.float, device=output_spotting.device)-1\n","\n","        for i in np.arange(output_spotting.size()[1]):\n","            tmp_timestamps[torch.floor(output_spotting[batch,i,1]*(chunk_size-1)).type(torch.int) , torch.argmax(output_spotting[batch,i,2:]).type(torch.int) ] = output_spotting[batch,i,0]\n","\n","        # ------------------------------------------\n","        # Store the result of the chunk in the video\n","        # ------------------------------------------\n","\n","        # For the first chunk\n","        if start == 0:\n","            timestamps_long[0:chunk_size-receptive_field] = tmp_timestamps[0:chunk_size-receptive_field]\n","\n","        # For the last chunk\n","        elif last:\n","            timestamps_long[start+receptive_field:start+chunk_size] = tmp_timestamps[receptive_field:]\n","            break\n","\n","        # For every other chunk\n","        else:\n","            timestamps_long[start+receptive_field:start+chunk_size-receptive_field] = tmp_timestamps[receptive_field:chunk_size-receptive_field]\n","\n","        # ---------------\n","        # Loop Management\n","        # ---------------\n","\n","        # Update the index\n","        start += chunk_size - 2 * receptive_field\n","        # Check if we are at the last index of the game\n","        if start + chunk_size >= video_size:\n","            start = video_size - chunk_size\n","            last = True\n","    return timestamps_long\n","\n","# Function to transform the batches to vectors\n","def batch2long(output_segmentation, video_size, chunk_size, receptive_field):\n","\n","    start = 0\n","    last = False\n","    receptive_field = receptive_field//2\n","\n","    segmentation_long = torch.zeros([video_size,output_segmentation.size()[-1]], dtype = torch.float, device=output_segmentation.device)\n","\n","\n","    for batch in np.arange(output_segmentation.size()[0]):\n","\n","        tmp_segmentation = torch.nn.functional.one_hot(torch.argmax(output_segmentation[batch], dim=-1), num_classes=output_segmentation.size()[-1])\n","\n","\n","        # ------------------------------------------\n","        # Store the result of the chunk in the video\n","        # ------------------------------------------\n","\n","        # For the first chunk\n","        if start == 0:\n","            segmentation_long[0:chunk_size-receptive_field] = tmp_segmentation[0:chunk_size-receptive_field]\n","\n","        # For the last chunk\n","        elif last:\n","            segmentation_long[start+receptive_field:start+chunk_size] = tmp_segmentation[receptive_field:]\n","            break\n","\n","        # For every other chunk\n","        else:\n","            segmentation_long[start+receptive_field:start+chunk_size-receptive_field] = tmp_segmentation[receptive_field:chunk_size-receptive_field]\n","\n","        # ---------------\n","        # Loop Management\n","        # ---------------\n","\n","        # Update the index\n","        start += chunk_size - 2 * receptive_field\n","        # Check if we are at the last index of the game\n","        if start + chunk_size >= video_size:\n","            start = video_size - chunk_size\n","            last = True\n","    return segmentation_long"],"metadata":{"id":"Bv86H35ImkcX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## dataset.py"],"metadata":{"id":"JomWpuBfm7gc"}},{"cell_type":"code","source":["# dataset.py\n","\n","from torch.utils.data import Dataset\n","\n","import numpy as np\n","import random\n","# import pandas as pd\n","import os\n","import time\n","\n","\n","from tqdm import tqdm\n","# import utils\n","\n","import torch\n","\n","import logging\n","import json\n","\n","from SoccerNet.Downloader import getListGames\n","from SoccerNet.Downloader import SoccerNetDownloader\n","\n","\n","\n","class SoccerNetClips(Dataset):\n","    def __init__(self, path, features=\"ResNET_PCA512.npy\", split=\"train\",\n","                framerate=2, chunk_size=240, receptive_field=80, chunks_per_epoch=6000):\n","        self.path = path\n","        self.listGames = getListGames(split)\n","        self.features = features\n","        self.chunk_size = chunk_size\n","        self.receptive_field = receptive_field\n","        self.chunks_per_epoch = chunks_per_epoch\n","\n","        self.dict_event = EVENT_DICTIONARY_V2\n","        self.num_classes = 17\n","        self.labels=\"Labels-v2.json\"\n","        self.K_parameters = K_V2*framerate\n","        self.num_detections =15\n","        self.split=split\n","\n","        logging.info(\"Checking/Download features and labels locally\")\n","        downloader = SoccerNetDownloader(path)\n","        downloader.downloadGames(files=[self.labels, f\"1_{self.features}\", f\"2_{self.features}\"], split=[split], verbose=False)\n","\n","\n","        logging.info(\"Pre-compute clips\")\n","\n","        clip_feats = []\n","        clip_labels = []\n","\n","        self.game_feats = list()\n","        self.game_labels = list()\n","        self.game_anchors = list()\n","        for i in np.arange(self.num_classes+1):\n","            self.game_anchors.append(list())\n","\n","        game_counter = 0\n","        for game in tqdm(self.listGames):\n","            # Load features\n","            feat_half1 = np.load(os.path.join(self.path, game, \"1_\" + self.features))\n","            feat_half2 = np.load(os.path.join(self.path, game, \"2_\" + self.features))\n","\n","            # Load labels\n","            labels = json.load(open(os.path.join(self.path, game, self.labels)))\n","\n","            label_half1 = np.zeros((feat_half1.shape[0], self.num_classes))\n","            label_half2 = np.zeros((feat_half2.shape[0], self.num_classes))\n","\n","\n","            for annotation in labels[\"annotations\"]:\n","\n","                time = annotation[\"gameTime\"]\n","                event = annotation[\"label\"]\n","\n","                half = int(time[0])\n","\n","                minutes = int(time[-5:-3])\n","                seconds = int(time[-2::])\n","                frame = framerate * ( seconds + 60 * minutes )\n","\n","                if event not in self.dict_event:\n","                    continue\n","                label = self.dict_event[event]\n","\n","\n","\n","                if half == 1:\n","                    frame = min(frame, feat_half1.shape[0]-1)\n","                    label_half1[frame][label] = 1\n","\n","                if half == 2:\n","                    frame = min(frame, feat_half2.shape[0]-1)\n","                    label_half2[frame][label] = 1\n","\n","            shift_half1 = oneHotToShifts(label_half1, self.K_parameters.cpu().numpy())\n","            shift_half2 = oneHotToShifts(label_half2, self.K_parameters.cpu().numpy())\n","\n","            anchors_half1 = getChunks_anchors(shift_half1, game_counter, self.K_parameters.cpu().numpy(), self.chunk_size, self.receptive_field)\n","\n","            game_counter = game_counter+1\n","\n","            anchors_half2 = getChunks_anchors(shift_half2, game_counter, self.K_parameters.cpu().numpy(), self.chunk_size, self.receptive_field)\n","\n","            game_counter = game_counter+1\n","\n","\n","\n","            self.game_feats.append(feat_half1)\n","            self.game_feats.append(feat_half2)\n","            self.game_labels.append(shift_half1)\n","            self.game_labels.append(shift_half2)\n","            for anchor in anchors_half1:\n","                self.game_anchors[anchor[2]].append(anchor)\n","            for anchor in anchors_half2:\n","                self.game_anchors[anchor[2]].append(anchor)\n","\n","\n","\n","    def __getitem__(self, index):\n","\n","        # Retrieve the game index and the anchor\n","        class_selection = random.randint(0, self.num_classes)\n","        event_selection = random.randint(0, len(self.game_anchors[class_selection])-1)\n","        game_index = self.game_anchors[class_selection][event_selection][0]\n","        anchor = self.game_anchors[class_selection][event_selection][1]\n","\n","        # Compute the shift for event chunks\n","        if class_selection < self.num_classes:\n","            shift = np.random.randint(-self.chunk_size+self.receptive_field, -self.receptive_field)\n","            start = anchor + shift\n","        # Compute the shift for non-event chunks\n","        else:\n","            start = random.randint(anchor[0], anchor[1]-self.chunk_size)\n","        if start < 0:\n","            start = 0\n","        if start+self.chunk_size >= self.game_feats[game_index].shape[0]:\n","            start = self.game_feats[game_index].shape[0]-self.chunk_size-1\n","\n","        # Extract the clips\n","        clip_feat = self.game_feats[game_index][start:start+self.chunk_size]\n","        clip_labels = self.game_labels[game_index][start:start+self.chunk_size]\n","\n","        # Put loss to zero outside receptive field\n","        clip_labels[0:int(np.ceil(self.receptive_field/2)),:] = -1\n","        clip_labels[-int(np.ceil(self.receptive_field/2)):,:] = -1\n","\n","        # Get the spotting target\n","        clip_targets = getTimestampTargets(np.array([clip_labels]), self.num_detections)[0]\n","\n","\n","        return torch.from_numpy(clip_feat), torch.from_numpy(clip_labels), torch.from_numpy(clip_targets)\n","\n","    def __len__(self):\n","        return self.chunks_per_epoch\n","\n","\n","class SoccerNetClipsTesting(Dataset):\n","    def __init__(self, path, features=\"ResNET_PCA512.npy\", split=\"test\",\n","                framerate=2, chunk_size=240, receptive_field=80):\n","        self.path = path\n","        self.listGames = getListGames(split)\n","        self.features = features\n","        self.chunk_size = chunk_size\n","        self.receptive_field = receptive_field\n","        self.framerate = framerate\n","\n","        self.dict_event = EVENT_DICTIONARY_V2\n","        self.num_classes = 17\n","        self.labels=\"Labels-v2.json\"\n","        self.K_parameters = K_V2*framerate\n","        self.num_detections =15\n","        self.split=split\n","\n","        logging.info(\"Checking/Download features and labels locally\")\n","        downloader = SoccerNetDownloader(path)\n","        if split == \"challenge\":\n","            downloader.downloadGames(files=[f\"1_{self.features}\", f\"2_{self.features}\"], split=[split], verbose=False)\n","        else:\n","            downloader.downloadGames(files=[self.labels, f\"1_{self.features}\", f\"2_{self.features}\"], split=[split], verbose=False)\n","\n","\n","\n","    def __getitem__(self, index):\n","\n","        # Load features\n","        feat_half1 = np.load(os.path.join(self.path, self.listGames[index], \"1_\" + self.features))\n","        feat_half2 = np.load(os.path.join(self.path, self.listGames[index], \"2_\" + self.features))\n","\n","\n","        label_half1 = np.zeros((feat_half1.shape[0], self.num_classes))\n","        label_half2 = np.zeros((feat_half2.shape[0], self.num_classes))\n","\n","\n","        # Load labels\n","        if os.path.exists(os.path.join(self.path, self.listGames[index], self.labels)):\n","            labels = json.load(open(os.path.join(self.path, self.listGames[index], self.labels)))\n","\n","            for annotation in labels[\"annotations\"]:\n","\n","                time = annotation[\"gameTime\"]\n","                event = annotation[\"label\"]\n","\n","                half = int(time[0])\n","\n","                minutes = int(time[-5:-3])\n","                seconds = int(time[-2::])\n","                frame = self.framerate * ( seconds + 60 * minutes )\n","\n","                if event not in self.dict_event:\n","                    continue\n","                label = self.dict_event[event]\n","\n","                value = 1\n","                if \"visibility\" in annotation.keys():\n","                    if annotation[\"visibility\"] == \"not shown\":\n","                        value = -1\n","\n","                if half == 1:\n","                    frame = min(frame, feat_half1.shape[0]-1)\n","                    label_half1[frame][label] = value\n","\n","                if half == 2:\n","                    frame = min(frame, feat_half2.shape[0]-1)\n","                    label_half2[frame][label] = value\n","\n","        def feats2clip(feats, stride, clip_length):\n","\n","            idx = torch.arange(start=0, end=feats.shape[0]-1, step=stride)\n","            idxs = []\n","            for i in torch.arange(0, clip_length):\n","                idxs.append(idx+i)\n","            idx = torch.stack(idxs, dim=1)\n","\n","            idx = idx.clamp(0, feats.shape[0]-1)\n","            idx[-1] = torch.arange(clip_length)+feats.shape[0]-clip_length\n","\n","            return feats[idx,:]\n","\n","\n","        feat_half1 = feats2clip(torch.from_numpy(feat_half1),\n","                        stride=self.chunk_size-self.receptive_field,\n","                        clip_length=self.chunk_size)\n","\n","        feat_half2 = feats2clip(torch.from_numpy(feat_half2),\n","                        stride=self.chunk_size-self.receptive_field,\n","                        clip_length=self.chunk_size)\n","\n","        return feat_half1, feat_half2, torch.from_numpy(label_half1), torch.from_numpy(label_half2)\n","\n","    def __len__(self):\n","        return len(self.listGames)"],"metadata":{"id":"yqyndMFmmmYk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## models.py"],"metadata":{"id":"Pe5WUErCnXaE"}},{"cell_type":"code","source":["# models.py\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","import math\n","\n","class SEBlock(nn.Module):\n","    def __init__(self, in_channels, reduction=4):\n","        super(SEBlock, self).__init__()\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # Global pooling over spatial/temporal dimensions\n","        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n","        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        batch, channels, _, _ = x.size()\n","        squeeze = self.global_avg_pool(x).view(batch, channels)\n","        excitation = self.fc2(F.relu(self.fc1(squeeze)))\n","        excitation = self.sigmoid(excitation).view(batch, channels, 1, 1)\n","        return x + (x * excitation)\n","\n","class ContextAwareModelSeb(nn.Module):\n","    def __init__(self, weights=None, input_size=512, num_classes=3, chunk_size=240, dim_capsule=16,\n","                 receptive_field=80, num_detections=5, framerate=2):\n","        super(ContextAwareModelSeb, self).__init__()\n","\n","        self.load_weights(weights=weights)\n","\n","        self.input_size = input_size\n","        self.num_classes = num_classes\n","        self.dim_capsule = dim_capsule\n","        self.receptive_field = receptive_field\n","        self.num_detections = num_detections\n","        self.chunk_size = chunk_size\n","        self.framerate = framerate\n","\n","        self.pyramid_size_1 = int(np.ceil(receptive_field / 7))\n","        self.pyramid_size_2 = int(np.ceil(receptive_field / 3))\n","        self.pyramid_size_3 = int(np.ceil(receptive_field / 2))\n","        self.pyramid_size_4 = int(np.ceil(receptive_field))\n","\n","        # Base Convolutional Layers\n","        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(1, input_size))\n","        self.conv_2 = nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1))\n","\n","        # Temporal Pyramidal Module\n","        self.pad_p_1 = nn.ZeroPad2d((0, 0, (self.pyramid_size_1 - 1) // 2, self.pyramid_size_1 - 1 - (self.pyramid_size_1 - 1) // 2))\n","        self.pad_p_2 = nn.ZeroPad2d((0, 0, (self.pyramid_size_2 - 1) // 2, self.pyramid_size_2 - 1 - (self.pyramid_size_2 - 1) // 2))\n","        self.pad_p_3 = nn.ZeroPad2d((0, 0, (self.pyramid_size_3 - 1) // 2, self.pyramid_size_3 - 1 - (self.pyramid_size_3 - 1) // 2))\n","        self.pad_p_4 = nn.ZeroPad2d((0, 0, (self.pyramid_size_4 - 1) // 2, self.pyramid_size_4 - 1 - (self.pyramid_size_4 - 1) // 2))\n","\n","        self.conv_p_1 = nn.Conv2d(in_channels=32, out_channels=8, kernel_size=(self.pyramid_size_1, 1), groups=8)\n","        self.conv_p_2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(self.pyramid_size_2, 1), groups=16)\n","        self.conv_p_3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(self.pyramid_size_3, 1), groups=32)\n","        self.conv_p_4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(self.pyramid_size_4, 1), groups=32)\n","\n","        # self.conv_p_1 = nn.Conv2d(in_channels=32, out_channels=8, kernel_size=(self.pyramid_size_1, 1))\n","        # self.conv_p_2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(self.pyramid_size_2, 1))\n","        # self.conv_p_3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(self.pyramid_size_3, 1))\n","        # self.conv_p_4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(self.pyramid_size_4, 1))\n","\n","        # SE Blocks for Attention\n","        self.se_p_1 = SEBlock(in_channels=8)\n","        self.se_p_2 = SEBlock(in_channels=16)\n","        self.se_p_3 = SEBlock(in_channels=32)\n","        self.se_p_4 = SEBlock(in_channels=64)\n","\n","        # self.ta_p_1 = TemporalAttention(in_channels=8, chunk_size=self.chunk_size)\n","        # self.ta_p_2 = TemporalAttention(in_channels=16, chunk_size=self.chunk_size)\n","        # self.ta_p_3 = TemporalAttention(in_channels=32, chunk_size=self.chunk_size)\n","        # self.ta_p_4 = TemporalAttention(in_channels=64, chunk_size=self.chunk_size)\n","\n","        # Add Dropout layers\n","        self.dropout_seg = nn.Dropout(p=0.3)\n","        self.dropout_spot = nn.Dropout(p=0.3)\n","\n","        # Batch Normalization for Segmentation\n","        self.conv_seg_bn = nn.BatchNorm2d(num_features=dim_capsule * num_classes)\n","\n","        # Segmentation Module\n","        self.kernel_seg_size = 3\n","        self.pad_seg = nn.ZeroPad2d((0, 0, (self.kernel_seg_size - 1) // 2, self.kernel_seg_size - 1 - (self.kernel_seg_size - 1) // 2))\n","        self.conv_seg = nn.Conv2d(in_channels=152, out_channels=dim_capsule * num_classes, kernel_size=(self.kernel_seg_size, 1))\n","        self.batch_seg = nn.BatchNorm2d(num_features=self.chunk_size, momentum=0.01, eps=0.001)\n","\n","        # Detection Module\n","        self.max_pool_spot = nn.MaxPool2d(kernel_size=(3, 1), stride=(2, 1))\n","        self.kernel_spot_size = 3\n","        self.pad_spot_1 = nn.ZeroPad2d((0, 0, (self.kernel_spot_size - 1) // 2, self.kernel_spot_size - 1 - (self.kernel_spot_size - 1) // 2))\n","        self.conv_spot_1 = nn.Conv2d(in_channels=num_classes * (dim_capsule + 1), out_channels=32, kernel_size=(self.kernel_spot_size, 1))\n","        self.max_pool_spot_1 = nn.MaxPool2d(kernel_size=(3, 1), stride=(2, 1))\n","        self.pad_spot_2 = nn.ZeroPad2d((0, 0, (self.kernel_spot_size - 1) // 2, self.kernel_spot_size - 1 - (self.kernel_spot_size - 1) // 2))\n","        self.conv_spot_2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(self.kernel_spot_size, 1))\n","        self.max_pool_spot_2 = nn.MaxPool2d(kernel_size=(3, 1), stride=(2, 1))\n","\n","        # Confidence Branch\n","        self.conv_conf = nn.Conv2d(in_channels=16 * (chunk_size // 8 - 1), out_channels=self.num_detections * 2, kernel_size=(1, 1))\n","\n","        # Class Branch\n","        self.conv_class = nn.Conv2d(in_channels=16 * (chunk_size // 8 - 1), out_channels=self.num_detections * self.num_classes, kernel_size=(1, 1))\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def load_weights(self, weights=None):\n","        if weights is not None:\n","            print(f\"=> loading checkpoint '{weights}'\")\n","            checkpoint = torch.load(weights)\n","            self.load_state_dict(checkpoint['state_dict'])\n","            print(f\"=> loaded checkpoint '{weights}' (epoch {checkpoint['epoch']})\")\n","\n","    def forward(self, inputs):\n","        # Forward pass through base layers\n","        conv_1 = F.relu(self.conv_1(inputs))\n","        conv_2 = F.relu(self.conv_2(conv_1))\n","\n","        # Temporal Pyramid Module with SE Blocks\n","        conv_p_1 = self.se_p_1(F.relu(self.conv_p_1(self.pad_p_1(conv_2))))\n","        conv_p_2 = self.se_p_2(F.relu(self.conv_p_2(self.pad_p_2(conv_2))))\n","        conv_p_3 = self.se_p_3(F.relu(self.conv_p_3(self.pad_p_3(conv_2))))\n","        conv_p_4 = self.se_p_4(F.relu(self.conv_p_4(self.pad_p_4(conv_2))))\n","        concatenation = torch.cat((conv_2, conv_p_1, conv_p_2, conv_p_3, conv_p_4), 1)\n","\n","        # Segmentation Module\n","        conv_seg = self.conv_seg(self.pad_seg(concatenation))\n","        # conv_seg = self.conv_seg_bn(conv_seg)  # Add BatchNorm\n","        # conv_seg = self.dropout_seg(conv_seg)  # Add Dropout\n","        conv_seg_permuted = conv_seg.permute(0, 2, 3, 1)\n","        conv_seg_reshaped = conv_seg_permuted.view(conv_seg_permuted.size()[0], conv_seg_permuted.size()[1], self.dim_capsule, self.num_classes)\n","        conv_seg_norm = torch.sigmoid(self.batch_seg(conv_seg_reshaped))\n","        output_segmentation = torch.sqrt(torch.sum(torch.square(conv_seg_norm - 0.5), dim=2) * 4 / self.dim_capsule)\n","\n","        # Spotting Module\n","        output_segmentation_reverse = 1 - output_segmentation\n","        output_segmentation_reverse_reshaped = output_segmentation_reverse.unsqueeze(2).permute(0, 3, 1, 2)\n","        concatenation_2 = torch.cat((conv_seg, output_segmentation_reverse_reshaped), dim=1)\n","        conv_spot = self.max_pool_spot(F.relu(concatenation_2))\n","        # conv_spot = self.dropout_spot(conv_spot)  # Add Dropout\n","        conv_spot_1 = F.relu(self.conv_spot_1(self.pad_spot_1(conv_spot)))\n","        conv_spot_1_pooled = self.max_pool_spot_1(conv_spot_1)\n","        conv_spot_2 = F.relu(self.conv_spot_2(self.pad_spot_2(conv_spot_1_pooled)))\n","        conv_spot_2_pooled = self.max_pool_spot_2(conv_spot_2)\n","        spotting_reshaped = conv_spot_2_pooled.view(conv_spot_2_pooled.size()[0], -1, 1, 1)\n","\n","        # Confidence Branch\n","        conf_pred = torch.sigmoid(self.conv_conf(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, 2))\n","\n","        # Class Branch\n","        conf_class = self.softmax(self.conv_class(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, self.num_classes))\n","        output_spotting = torch.cat((conf_pred, conf_class), dim=-1)\n","\n","        return output_segmentation, output_spotting\n","\n","    # def forward(self, inputs):\n","    #     # Base Convolutional Layers\n","    #     conv_1 = F.relu(self.conv_1(inputs))\n","    #     conv_2 = F.relu(self.conv_2(conv_1))\n","\n","    #     # Temporal Pyramidal Module\n","    #     conv_p_1 = F.relu(self.conv_p_1(self.pad_p_1(conv_2)))\n","    #     conv_p_2 = F.relu(self.conv_p_2(self.pad_p_2(conv_2)))\n","    #     conv_p_3 = F.relu(self.conv_p_3(self.pad_p_3(conv_2)))\n","    #     conv_p_4 = F.relu(self.conv_p_4(self.pad_p_4(conv_2)))\n","\n","    #     # SE Attention\n","    #     # restore if self_attention doesnt improve metrics\n","    #     conv_p_1 = self.se_p_1(conv_p_1)\n","    #     conv_p_2 = self.se_p_2(conv_p_2)\n","    #     conv_p_3 = self.se_p_3(conv_p_3)\n","    #     conv_p_4 = self.se_p_4(conv_p_4)\n","\n","    #     # conv_p_1 = self.ta_p_1(self.se_p_1(conv_p_1))\n","    #     # conv_p_2 = self.ta_p_2(self.se_p_2(conv_p_2))\n","    #     # conv_p_3 = self.ta_p_3(self.se_p_3(conv_p_3))\n","    #     # conv_p_4 = self.ta_p_4(self.se_p_4(conv_p_4))\n","\n","    #     concatenation = torch.cat((conv_2, conv_p_1, conv_p_2, conv_p_3, conv_p_4), 1)\n","\n","    #     # Segmentation Module\n","    #     conv_seg = self.conv_seg(self.pad_seg(concatenation))\n","    #     conv_seg_permuted = conv_seg.permute(0, 2, 3, 1)\n","    #     conv_seg_reshaped = conv_seg_permuted.view(conv_seg_permuted.size()[0], conv_seg_permuted.size()[1], self.dim_capsule, self.num_classes)\n","    #     conv_seg_norm = torch.sigmoid(self.batch_seg(conv_seg_reshaped))\n","    #     output_segmentation = torch.sqrt(torch.sum(torch.square(conv_seg_norm - 0.5), dim=2) * 4 / self.dim_capsule)\n","\n","    #     # Spotting Module\n","    #     output_segmentation_reverse = 1 - output_segmentation\n","    #     output_segmentation_reverse_reshaped = output_segmentation_reverse.unsqueeze(2).permute(0, 3, 1, 2)\n","    #     concatenation_2 = torch.cat((conv_seg, output_segmentation_reverse_reshaped), dim=1)\n","    #     conv_spot = self.max_pool_spot(F.relu(concatenation_2))\n","    #     conv_spot_1 = F.relu(self.conv_spot_1(self.pad_spot_1(conv_spot)))\n","    #     conv_spot_1_pooled = self.max_pool_spot_1(conv_spot_1)\n","    #     conv_spot_2 = F.relu(self.conv_spot_2(self.pad_spot_2(conv_spot_1_pooled)))\n","    #     conv_spot_2_pooled = self.max_pool_spot_2(conv_spot_2)\n","    #     spotting_reshaped = conv_spot_2_pooled.view(conv_spot_2_pooled.size()[0], -1, 1, 1)\n","\n","    #     # Confidence Branch\n","    #     conf_pred = torch.sigmoid(self.conv_conf(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, 2))\n","\n","    #     # Class Branch\n","    #     conf_class = self.softmax(self.conv_class(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, self.num_classes))\n","    #     output_spotting = torch.cat((conf_pred, conf_class), dim=-1)\n","\n","    #     return output_segmentation, output_spotting\n","\n","class FixedPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(FixedPositionalEncoding, self).__init__()\n","        # Create fixed positional encodings\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n","        self.register_buffer('pe', pe)  # Not a parameter, but persists with the model\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Add positional encodings to input tensor.\n","        Args:\n","            x: Tensor of shape (batch_size, seq_len, d_model)\n","        Returns:\n","            Tensor of shape (batch_size, seq_len, d_model) with positional encodings added\n","        \"\"\"\n","        return x + self.pe[:, :x.size(1)]  # Slice encodings to match sequence length\n","\n","\n","# class PositionalEncoding(nn.Module):\n","#     def __init__(self, d_model, max_len=5000):\n","#         super(PositionalEncoding, self).__init__()\n","#         pe = torch.zeros(max_len, d_model)\n","#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","#         pe[:, 0::2] = torch.sin(position * div_term)\n","#         pe[:, 1::2] = torch.cos(position * div_term)\n","#         pe = pe.unsqueeze(0)  # Add batch dimension\n","#         self.register_buffer('pe', pe)\n","\n","#     def forward(self, x):\n","#         \"\"\"\n","#         x: (batch, seq_len, d_model)\n","#         \"\"\"\n","#         x = x + self.pe[:, :x.size(1), :]\n","#         return x\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)  # Add batch dimension\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (batch, seq_len, d_model)\n","        \"\"\"\n","        # Ensure `self.pe` is on the same device as `x`\n","        pe = self.pe[:, :x.size(1), :].to(x.device)\n","        return x + pe\n","\n","class CustomTransformerEncoder(nn.Module):\n","    def __init__(self, input_dim, num_heads, ff_dim, dropout=0.1):\n","        super(CustomTransformerEncoder, self).__init__()\n","        self.self_attn = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n","        self.ffn = nn.Sequential(\n","            nn.Linear(input_dim, ff_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(ff_dim, input_dim)\n","        )\n","        self.norm1 = nn.LayerNorm(input_dim)\n","        self.norm2 = nn.LayerNorm(input_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # x: (batch_size, chunk_size, channels)\n","\n","        # Self-Attention\n","        attn_output, _ = self.self_attn(x, x, x)  # (batch_size, chunk_size, channels)\n","        x = self.norm1(x + self.dropout(attn_output))\n","\n","        # Feed-Forward Network\n","        ffn_output = self.ffn(x)  # (batch_size, chunk_size, channels)\n","        x = self.norm2(x + self.dropout(ffn_output))\n","\n","        return x\n","\n","class ContextAwareModelTran(nn.Module):\n","    def __init__(self, weights=None, input_size=512, num_classes=3, chunk_size=240, dim_capsule=16, receptive_field=80, num_detections=5, framerate=2):\n","        super(ContextAwareModelTran, self).__init__()\n","\n","        self.load_weights(weights=weights)\n","\n","        self.input_size = input_size\n","        self.num_classes = num_classes\n","        self.dim_capsule = dim_capsule\n","        self.receptive_field = receptive_field\n","        self.num_detections = num_detections\n","        self.chunk_size = chunk_size\n","        self.framerate = framerate\n","\n","        # Transformer-based Temporal Module\n","        self.transformer_layer = TransformerEncoderLayer(\n","            d_model=input_size,  # Match feature dimension from inputs\n","            nhead=8,             # Number of attention heads\n","            dim_feedforward=2048,  # Feedforward size\n","            dropout=0.1,         # Dropout rate\n","            batch_first = True\n","        )\n","        self.transformer_encoder = TransformerEncoder(self.transformer_layer, num_layers=2)\n","\n","        # -------------------\n","        # Redesigned Segmentation Module\n","        # -------------------\n","        self.conv_seg = nn.Sequential(\n","            nn.Conv1d(\n","                in_channels=input_size,  # Match Transformer feature_dim\n","                out_channels=dim_capsule * num_classes,\n","                kernel_size=3,  # Temporal kernel\n","                padding=1       # Preserve temporal size\n","            ),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(dim_capsule * num_classes)  # Normalize output\n","        )\n","\n","        # -------------------\n","        # detection module\n","        # -------------------\n","        self.max_pool_spot = nn.MaxPool2d(kernel_size=(3,1),stride=(2,1))\n","        self.kernel_spot_size = 3\n","        self.pad_spot_1 = nn.ZeroPad2d((0,0,(self.kernel_spot_size-1)//2, self.kernel_spot_size-1-(self.kernel_spot_size-1)//2))\n","        self.conv_spot_1 = nn.Conv2d(in_channels=num_classes*(dim_capsule+1), out_channels=32, kernel_size=(self.kernel_spot_size,1))\n","        self.max_pool_spot_1 = nn.MaxPool2d(kernel_size=(3,1),stride=(2,1))\n","        self.pad_spot_2 = nn.ZeroPad2d((0,0,(self.kernel_spot_size-1)//2, self.kernel_spot_size-1-(self.kernel_spot_size-1)//2))\n","        self.conv_spot_2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(self.kernel_spot_size,1))\n","        self.max_pool_spot_2 = nn.MaxPool2d(kernel_size=(3,1),stride=(2,1))\n","\n","        # Confidence branch\n","        self.conv_conf = nn.Conv2d(in_channels=16*(chunk_size//8-1), out_channels=self.num_detections*2, kernel_size=(1,1))\n","\n","        # Class branch\n","        self.conv_class = nn.Conv2d(in_channels=16*(chunk_size//8-1), out_channels=self.num_detections*self.num_classes, kernel_size=(1,1))\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","\n","    def load_weights(self, weights=None):\n","        if(weights is not None):\n","            print(\"=> loading checkpoint '{}'\".format(weights))\n","            checkpoint = torch.load(weights)\n","            self.load_state_dict(checkpoint['state_dict'])\n","            print(\"=> loaded checkpoint '{}' (epoch {})\"\n","                  .format(weights, checkpoint['epoch']))\n","\n","    def forward(self, inputs):\n","        # Prepare the data for the Transformer\n","        inputs_flattened = inputs.squeeze(1)  # (batch, chunk_size, feature_dim)\n","\n","        # Apply Transformer\n","        transformed = self.transformer_encoder(inputs_flattened)  # (batch, chunk_size, feature_dim)\n","\n","        # Prepare for segmentation module\n","        transformed = transformed.permute(0, 2, 1)  # (batch, feature_dim, chunk_size)\n","\n","        # -------------------\n","        # Segmentation module\n","        # -------------------\n","        conv_seg = self.conv_seg(transformed)  # (batch, dim_capsule * num_classes, chunk_size)\n","\n","        # Reshape for capsule-like processing\n","        conv_seg_permuted = conv_seg.permute(0, 2, 1)  # (batch, chunk_size, dim_capsule * num_classes)\n","        conv_seg_reshaped = conv_seg_permuted.view(\n","            conv_seg_permuted.size(0),\n","            conv_seg_permuted.size(1),\n","            self.dim_capsule,\n","            self.num_classes\n","        )  # (batch, chunk_size, dim_capsule, num_classes)\n","\n","        conv_seg_norm = torch.sigmoid(conv_seg_reshaped)  # Normalize the capsules\n","        output_segmentation = torch.sqrt(torch.sum(torch.square(conv_seg_norm - 0.5), dim=2) * 4 / self.dim_capsule)\n","\n","        # -------------------\n","        # Spotting module\n","        # -------------------\n","        output_segmentation_reverse = 1 - output_segmentation\n","        output_segmentation_reverse_reshaped = output_segmentation_reverse.unsqueeze(2)\n","        output_segmentation_reverse_reshaped_permutted = output_segmentation_reverse_reshaped.permute(0, 3, 1, 2)\n","\n","        concatenation_2 = torch.cat((conv_seg.unsqueeze(-1), output_segmentation_reverse_reshaped_permutted), dim=1)\n","        conv_spot = self.max_pool_spot(F.relu(concatenation_2))\n","\n","        # Spotting processing\n","        conv_spot_1 = F.relu(self.conv_spot_1(self.pad_spot_1(conv_spot)))\n","        conv_spot_1_pooled = self.max_pool_spot_1(conv_spot_1)\n","        conv_spot_2 = F.relu(self.conv_spot_2(self.pad_spot_2(conv_spot_1_pooled)))\n","        conv_spot_2_pooled = self.max_pool_spot_2(conv_spot_2)\n","\n","        spotting_reshaped = conv_spot_2_pooled.view(conv_spot_2_pooled.size(0), -1, 1, 1)\n","        conf_pred = torch.sigmoid(self.conv_conf(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, 2))\n","        conf_class = self.softmax(self.conv_class(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, self.num_classes))\n","\n","        output_spotting = torch.cat((conf_pred, conf_class), dim=-1)\n","\n","        return output_segmentation, output_spotting\n","\n","class ContextAwareModelMHA(nn.Module):\n","    def __init__(self, weights=None, input_size=512, num_classes=3, chunk_size=240, dim_capsule=16,\n","                 receptive_field=80, num_detections=5, framerate=2, transformer_heads=4, transformer_ff_dim=512):\n","        super(ContextAwareModelMHA, self).__init__()\n","\n","        self.load_weights(weights=weights)\n","        self.input_size = input_size\n","        self.num_classes = num_classes\n","        self.dim_capsule = dim_capsule\n","        self.receptive_field = receptive_field\n","        self.num_detections = num_detections\n","        self.chunk_size = chunk_size\n","        self.framerate = framerate\n","\n","        self.pyramid_size_1 = int(np.ceil(receptive_field/7))\n","        self.pyramid_size_2 = int(np.ceil(receptive_field/3))\n","        self.pyramid_size_3 = int(np.ceil(receptive_field/2))\n","        self.pyramid_size_4 = int(np.ceil(receptive_field))\n","\n","        # Base Convolutional Layers\n","        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(1, input_size))\n","        self.conv_2 = nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1))\n","\n","        # Temporal Pyramidal Module\n","        self.pad_p_1 = nn.ZeroPad2d((0, 0, (self.pyramid_size_1-1)//2, self.pyramid_size_1-1-(self.pyramid_size_1-1)//2))\n","        self.pad_p_2 = nn.ZeroPad2d((0, 0, (self.pyramid_size_2-1)//2, self.pyramid_size_2-1-(self.pyramid_size_2-1)//2))\n","        self.pad_p_3 = nn.ZeroPad2d((0, 0, (self.pyramid_size_3-1)//2, self.pyramid_size_3-1-(self.pyramid_size_3-1)//2))\n","        self.pad_p_4 = nn.ZeroPad2d((0, 0, (self.pyramid_size_4-1)//2, self.pyramid_size_4-1-(self.pyramid_size_4-1)//2))\n","        self.conv_p_1 = nn.Conv2d(in_channels=32, out_channels=8, kernel_size=(self.pyramid_size_1, 1))\n","        self.conv_p_2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(self.pyramid_size_2, 1))\n","        self.conv_p_3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(self.pyramid_size_3, 1))\n","        self.conv_p_4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(self.pyramid_size_4, 1))\n","\n","        # Transformer\n","        self.transformer = CustomTransformerEncoder(input_dim=152, num_heads=transformer_heads, ff_dim=transformer_ff_dim)\n","        self.positional_encoding = PositionalEncoding(d_model=152)\n","\n","        # Segmentation Module\n","        self.kernel_seg_size = 3\n","        self.pad_seg = nn.ZeroPad2d((0, 0, (self.kernel_seg_size-1)//2, self.kernel_seg_size-1-(self.kernel_seg_size-1)//2))\n","        self.conv_seg = nn.Conv2d(in_channels=152, out_channels=dim_capsule * num_classes, kernel_size=(self.kernel_seg_size, 1))\n","        self.batch_seg = nn.BatchNorm2d(num_features=self.chunk_size, momentum=0.01, eps=0.001)\n","\n","        # Detection Module (unchanged from the original)\n","        self.max_pool_spot = nn.MaxPool2d(kernel_size=(3, 1), stride=(2, 1))\n","        self.kernel_spot_size = 3\n","        self.pad_spot_1 = nn.ZeroPad2d((0, 0, (self.kernel_spot_size-1)//2, self.kernel_spot_size-1-(self.kernel_spot_size-1)//2))\n","        self.conv_spot_1 = nn.Conv2d(in_channels=num_classes * (dim_capsule + 1), out_channels=32, kernel_size=(self.kernel_spot_size, 1))\n","        self.max_pool_spot_1 = nn.MaxPool2d(kernel_size=(3, 1), stride=(2, 1))\n","        self.pad_spot_2 = nn.ZeroPad2d((0, 0, (self.kernel_spot_size-1)//2, self.kernel_spot_size-1-(self.kernel_spot_size-1)//2))\n","        self.conv_spot_2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(self.kernel_spot_size, 1))\n","        self.max_pool_spot_2 = nn.MaxPool2d(kernel_size=(3, 1), stride=(2, 1))\n","        self.conv_conf = nn.Conv2d(in_channels=16 * (chunk_size // 8 - 1), out_channels=self.num_detections * 2, kernel_size=(1, 1))\n","        self.conv_class = nn.Conv2d(in_channels=16 * (chunk_size // 8 - 1), out_channels=self.num_detections * self.num_classes, kernel_size=(1, 1))\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def load_weights(self, weights=None):\n","        if weights is not None:\n","            checkpoint = torch.load(weights)\n","            self.load_state_dict(checkpoint['state_dict'])\n","\n","    def forward(self, inputs):\n","        # Base Convolutional Layers\n","        conv_1 = F.relu(self.conv_1(inputs))\n","        conv_2 = F.relu(self.conv_2(conv_1))\n","\n","        # Temporal Pyramidal Module\n","        conv_p_1 = F.relu(self.conv_p_1(self.pad_p_1(conv_2)))\n","        conv_p_2 = F.relu(self.conv_p_2(self.pad_p_2(conv_2)))\n","        conv_p_3 = F.relu(self.conv_p_3(self.pad_p_3(conv_2)))\n","        conv_p_4 = F.relu(self.conv_p_4(self.pad_p_4(conv_2)))\n","\n","        concatenation = torch.cat((conv_2, conv_p_1, conv_p_2, conv_p_3, conv_p_4), 1)  # (batch, channels, chunk_size, 1)\n","\n","        # Transformer\n","        transformer_input = concatenation.squeeze(-1).permute(0, 2, 1)  # (batch, chunk_size, channels)\n","        transformer_input = self.positional_encoding(transformer_input)  # Add positional encoding\n","        transformer_output = self.transformer(transformer_input)  # (batch, chunk_size, channels)\n","        transformer_output = transformer_output.permute(0, 2, 1).unsqueeze(-1)  # (batch, channels, chunk_size, 1)\n","\n","        # Segmentation Module\n","        conv_seg = self.conv_seg(self.pad_seg(transformer_output))\n","        conv_seg_permuted = conv_seg.permute(0, 2, 3, 1)\n","        conv_seg_reshaped = conv_seg_permuted.view(conv_seg_permuted.size(0), conv_seg_permuted.size(1), self.dim_capsule, self.num_classes)\n","        conv_seg_norm = torch.sigmoid(self.batch_seg(conv_seg_reshaped))\n","        output_segmentation = torch.sqrt(torch.sum(torch.square(conv_seg_norm - 0.5), dim=2) * 4 / self.dim_capsule)\n","\n","        # Detection Module (unchanged)\n","        output_segmentation_reverse = 1 - output_segmentation\n","        output_segmentation_reverse_reshaped = output_segmentation_reverse.unsqueeze(2).permute(0, 3, 1, 2)\n","        concatenation_2 = torch.cat((conv_seg, output_segmentation_reverse_reshaped), dim=1)\n","        conv_spot = self.max_pool_spot(F.relu(concatenation_2))\n","        conv_spot_1 = F.relu(self.conv_spot_1(self.pad_spot_1(conv_spot)))\n","        conv_spot_1_pooled = self.max_pool_spot_1(conv_spot_1)\n","        conv_spot_2 = F.relu(self.conv_spot_2(self.pad_spot_2(conv_spot_1_pooled)))\n","        conv_spot_2_pooled = self.max_pool_spot_2(conv_spot_2)\n","        spotting_reshaped = conv_spot_2_pooled.view(conv_spot_2_pooled.size(0), -1, 1, 1)\n","        conf_pred = torch.sigmoid(self.conv_conf(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, 2))\n","        conf_class = self.softmax(self.conv_class(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, self.num_classes))\n","        output_spotting = torch.cat((conf_pred, conf_class), dim=-1)\n","\n","        return output_segmentation, output_spotting\n","\n","class ContextAwareModelHybrid(nn.Module):\n","    def __init__(self, weights=None, input_size=512, num_classes=3, chunk_size=240, dim_capsule=16, receptive_field=80, num_detections=5, framerate=2):\n","        super(ContextAwareModelHybrid, self).__init__()\n","\n","        self.load_weights(weights=weights)\n","\n","        self.input_size = input_size\n","        self.num_classes = num_classes\n","        self.dim_capsule = dim_capsule\n","        self.receptive_field = receptive_field\n","        self.num_detections = num_detections\n","        self.chunk_size = chunk_size\n","        self.framerate = framerate\n","\n","        #input size = 32, 1, 240, 512\n","\n","        # Temporal CNN Block\n","        self.temporal_cnn = nn.Sequential(\n","            nn.Conv1d(\n","                in_channels=input_size,  # Feature dimension from input\n","                out_channels=256,       # Number of filters\n","                kernel_size=3,          # Temporal kernel\n","                padding=1               # Preserve temporal size\n","            ),\n","            nn.ReLU(),\n","            nn.Conv1d(\n","                in_channels=256,\n","                out_channels=512,\n","                kernel_size=3,\n","                padding=1\n","            ),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(512)         # Normalize temporal features\n","        )\n","\n","        # Transformer-based Temporal Module\n","        self.transformer_layer = TransformerEncoderLayer(\n","            d_model=input_size,  # Match feature dimension from inputs\n","            nhead=8,             # Number of attention heads\n","            dim_feedforward=2048,  # Feedforward size\n","            dropout=0.1,         # Dropout rate\n","            batch_first = True\n","        )\n","        self.transformer_encoder = TransformerEncoder(self.transformer_layer, num_layers=2)\n","\n","        # -------------------\n","        # Redesigned Segmentation Module\n","        # -------------------\n","        self.conv_seg = nn.Sequential(\n","            nn.Conv1d(\n","                in_channels=input_size,  # Match Transformer feature_dim\n","                out_channels=dim_capsule * num_classes,\n","                kernel_size=3,  # Temporal kernel\n","                padding=1       # Preserve temporal size\n","            ),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(dim_capsule * num_classes)  # Normalize output\n","        )\n","\n","        # -------------------\n","        # detection module\n","        # -------------------\n","        self.max_pool_spot = nn.MaxPool2d(kernel_size=(3,1),stride=(2,1))\n","        self.kernel_spot_size = 3\n","        self.pad_spot_1 = nn.ZeroPad2d((0,0,(self.kernel_spot_size-1)//2, self.kernel_spot_size-1-(self.kernel_spot_size-1)//2))\n","        self.conv_spot_1 = nn.Conv2d(in_channels=num_classes*(dim_capsule+1), out_channels=32, kernel_size=(self.kernel_spot_size,1))\n","        self.max_pool_spot_1 = nn.MaxPool2d(kernel_size=(3,1),stride=(2,1))\n","        self.pad_spot_2 = nn.ZeroPad2d((0,0,(self.kernel_spot_size-1)//2, self.kernel_spot_size-1-(self.kernel_spot_size-1)//2))\n","        self.conv_spot_2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(self.kernel_spot_size,1))\n","        self.max_pool_spot_2 = nn.MaxPool2d(kernel_size=(3,1),stride=(2,1))\n","\n","        # Confidence branch\n","        self.conv_conf = nn.Conv2d(in_channels=16*(chunk_size//8-1), out_channels=self.num_detections*2, kernel_size=(1,1))\n","\n","        # Class branch\n","        self.conv_class = nn.Conv2d(in_channels=16*(chunk_size//8-1), out_channels=self.num_detections*self.num_classes, kernel_size=(1,1))\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","\n","    def load_weights(self, weights=None):\n","        if(weights is not None):\n","            print(\"=> loading checkpoint '{}'\".format(weights))\n","            checkpoint = torch.load(weights)\n","            self.load_state_dict(checkpoint['state_dict'])\n","            print(\"=> loaded checkpoint '{}' (epoch {})\"\n","                  .format(weights, checkpoint['epoch']))\n","\n","    def forward(self, inputs):\n","        # Prepare the data for the Transformer\n","        inputs_flattened = inputs.squeeze(1)  # (batch, chunk_size, feature_dim)\n","        inputs_flattened = inputs_flattened.permute(0, 2, 1)  # (batch, feature_dim, chunk_size)\n","\n","        # Temporal CNN Block\n","        temporal_cnn_output = self.temporal_cnn(inputs_flattened) # (batch, 512, chunk_size)\n","\n","        #prepare for transformer\n","        transformer_input = temporal_cnn_output.permute(0, 2, 1) # (batch, chunk_size, 512)\n","\n","        # Apply Transformer\n","        transformed = self.transformer_encoder(transformer_input)  # (batch, chunk_size, feature_dim)\n","\n","        # Prepare for segmentation module\n","        transformed = transformed.permute(0, 2, 1)  # (batch, feature_dim, chunk_size)\n","\n","        # -------------------\n","        # Segmentation module\n","        # -------------------\n","        conv_seg = self.conv_seg(transformed)  # (batch, dim_capsule * num_classes, chunk_size)\n","\n","        # Reshape for capsule-like processing\n","        conv_seg_permuted = conv_seg.permute(0, 2, 1)  # (batch, chunk_size, dim_capsule * num_classes)\n","        conv_seg_reshaped = conv_seg_permuted.view(\n","            conv_seg_permuted.size(0),\n","            conv_seg_permuted.size(1),\n","            self.dim_capsule,\n","            self.num_classes\n","        )  # (batch, chunk_size, dim_capsule, num_classes)\n","\n","        conv_seg_norm = torch.sigmoid(conv_seg_reshaped)  # Normalize the capsules\n","        output_segmentation = torch.sqrt(torch.sum(torch.square(conv_seg_norm - 0.5), dim=2) * 4 / self.dim_capsule)\n","\n","        # -------------------\n","        # Spotting module\n","        # -------------------\n","        output_segmentation_reverse = 1 - output_segmentation\n","        output_segmentation_reverse_reshaped = output_segmentation_reverse.unsqueeze(2)\n","        output_segmentation_reverse_reshaped_permutted = output_segmentation_reverse_reshaped.permute(0, 3, 1, 2)\n","\n","        concatenation_2 = torch.cat((conv_seg.unsqueeze(-1), output_segmentation_reverse_reshaped_permutted), dim=1)\n","        conv_spot = self.max_pool_spot(F.relu(concatenation_2))\n","\n","        # Spotting processing\n","        conv_spot_1 = F.relu(self.conv_spot_1(self.pad_spot_1(conv_spot)))\n","        conv_spot_1_pooled = self.max_pool_spot_1(conv_spot_1)\n","        conv_spot_2 = F.relu(self.conv_spot_2(self.pad_spot_2(conv_spot_1_pooled)))\n","        conv_spot_2_pooled = self.max_pool_spot_2(conv_spot_2)\n","\n","        spotting_reshaped = conv_spot_2_pooled.view(conv_spot_2_pooled.size(0), -1, 1, 1)\n","        conf_pred = torch.sigmoid(self.conv_conf(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, 2))\n","        conf_class = self.softmax(self.conv_class(spotting_reshaped).view(spotting_reshaped.shape[0], self.num_detections, self.num_classes))\n","\n","        output_spotting = torch.cat((conf_pred, conf_class), dim=-1)\n","\n","        return output_segmentation, output_spotting\n","\n","\n","class ContextAwareModel(nn.Module):\n","    def __init__(self, weights=None, input_size=512, num_classes=3, chunk_size=240, dim_capsule=16, receptive_field=80, num_detections=5, framerate=2):\n","        \"\"\"\n","        INPUT: a Tensor of the form (batch_size,1,chunk_size,input_size)\n","        OUTPUTS:    1. The segmentation of the form (batch_size,chunk_size,num_classes)\n","                    2. The action spotting of the form (batch_size,num_detections,2+num_classes)\n","        \"\"\"\n","\n","        super(ContextAwareModel, self).__init__()\n","\n","        self.load_weights(weights=weights)\n","\n","        self.input_size = input_size\n","        self.num_classes = num_classes\n","        self.dim_capsule = dim_capsule\n","        self.receptive_field = receptive_field\n","        self.num_detections = num_detections\n","        self.chunk_size = chunk_size\n","        self.framerate = framerate\n","\n","        self.pyramid_size_1 = int(np.ceil(receptive_field/7))\n","        self.pyramid_size_2 = int(np.ceil(receptive_field/3))\n","        self.pyramid_size_3 = int(np.ceil(receptive_field/2))\n","        self.pyramid_size_4 = int(np.ceil(receptive_field))\n","\n","        # Base Convolutional Layers\n","        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(1,input_size))\n","        self.conv_2 = nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1,1))\n","\n","        # Temporal Pyramidal Module\n","        self.pad_p_1 = nn.ZeroPad2d((0,0,(self.pyramid_size_1-1)//2, self.pyramid_size_1-1-(self.pyramid_size_1-1)//2))\n","        self.pad_p_2 = nn.ZeroPad2d((0,0,(self.pyramid_size_2-1)//2, self.pyramid_size_2-1-(self.pyramid_size_2-1)//2))\n","        self.pad_p_3 = nn.ZeroPad2d((0,0,(self.pyramid_size_3-1)//2, self.pyramid_size_3-1-(self.pyramid_size_3-1)//2))\n","        self.pad_p_4 = nn.ZeroPad2d((0,0,(self.pyramid_size_4-1)//2, self.pyramid_size_4-1-(self.pyramid_size_4-1)//2))\n","        self.conv_p_1 = nn.Conv2d(in_channels=32, out_channels=8, kernel_size=(self.pyramid_size_1,1))\n","        self.conv_p_2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(self.pyramid_size_2,1))\n","        self.conv_p_3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(self.pyramid_size_3,1))\n","        self.conv_p_4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(self.pyramid_size_4,1))\n","\n","        # -------------------\n","        # Segmentation module\n","        # -------------------\n","\n","        self.kernel_seg_size = 3\n","        self.pad_seg = nn.ZeroPad2d((0,0,(self.kernel_seg_size-1)//2, self.kernel_seg_size-1-(self.kernel_seg_size-1)//2))\n","        self.conv_seg = nn.Conv2d(in_channels=152, out_channels=dim_capsule*num_classes, kernel_size=(self.kernel_seg_size,1))\n","        self.batch_seg = nn.BatchNorm2d(num_features=self.chunk_size, momentum=0.01,eps=0.001)\n","\n","\n","        # -------------------\n","        # detection module\n","        # -------------------\n","        self.max_pool_spot = nn.MaxPool2d(kernel_size=(3,1),stride=(2,1))\n","        self.kernel_spot_size = 3\n","        self.pad_spot_1 = nn.ZeroPad2d((0,0,(self.kernel_spot_size-1)//2, self.kernel_spot_size-1-(self.kernel_spot_size-1)//2))\n","        self.conv_spot_1 = nn.Conv2d(in_channels=num_classes*(dim_capsule+1), out_channels=32, kernel_size=(self.kernel_spot_size,1))\n","        self.max_pool_spot_1 = nn.MaxPool2d(kernel_size=(3,1),stride=(2,1))\n","        self.pad_spot_2 = nn.ZeroPad2d((0,0,(self.kernel_spot_size-1)//2, self.kernel_spot_size-1-(self.kernel_spot_size-1)//2))\n","        self.conv_spot_2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(self.kernel_spot_size,1))\n","        self.max_pool_spot_2 = nn.MaxPool2d(kernel_size=(3,1),stride=(2,1))\n","\n","        # Confidence branch\n","        self.conv_conf = nn.Conv2d(in_channels=16*(chunk_size//8-1), out_channels=self.num_detections*2, kernel_size=(1,1))\n","\n","        # Class branch\n","        self.conv_class = nn.Conv2d(in_channels=16*(chunk_size//8-1), out_channels=self.num_detections*self.num_classes, kernel_size=(1,1))\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","\n","    def load_weights(self, weights=None):\n","        if(weights is not None):\n","            print(\"=> loading checkpoint '{}'\".format(weights))\n","            checkpoint = torch.load(weights)\n","            self.load_state_dict(checkpoint['state_dict'])\n","            print(\"=> loaded checkpoint '{}' (epoch {})\"\n","                  .format(weights, checkpoint['epoch']))\n","\n","    def forward(self, inputs):\n","\n","        # -----------------------------------\n","        # Feature input (chunks of the video)\n","        # -----------------------------------\n","        # input_shape: (batch,channel,frames,dim_features)\n","        #print(\"Input size: \", inputs.size())\n","\n","        # -------------------------------------\n","        # Temporal Convolutional neural network\n","        # -------------------------------------\n","\n","\n","        # Base Convolutional Layers\n","        conv_1 = F.relu(self.conv_1(inputs))\n","        #print(\"Conv_1 size: \", conv_1.size())\n","\n","        conv_2 = F.relu(self.conv_2(conv_1))\n","        #print(\"Conv_2 size: \", conv_2.size())\n","\n","\n","        # Temporal Pyramidal Module\n","        conv_p_1 = F.relu(self.conv_p_1(self.pad_p_1(conv_2)))\n","        #print(\"Conv_p_1 size: \", conv_p_1.size())\n","        conv_p_2 = F.relu(self.conv_p_2(self.pad_p_2(conv_2)))\n","        #print(\"Conv_p_2 size: \", conv_p_2.size())\n","        conv_p_3 = F.relu(self.conv_p_3(self.pad_p_3(conv_2)))\n","        #print(\"Conv_p_3 size: \", conv_p_3.size())\n","        conv_p_4 = F.relu(self.conv_p_4(self.pad_p_4(conv_2)))\n","        #print(\"Conv_p_4 size: \", conv_p_4.size())\n","\n","        concatenation = torch.cat((conv_2,conv_p_1,conv_p_2,conv_p_3,conv_p_4),1)\n","        #print(\"Concatenation size: \", concatenation.size())\n","\n","\n","        # -------------------\n","        # Segmentation module\n","        # -------------------\n","\n","        conv_seg = self.conv_seg(self.pad_seg(concatenation))\n","        #print(\"Conv_seg size: \", conv_seg.size())\n","\n","        conv_seg_permuted = conv_seg.permute(0,2,3,1)\n","        #print(\"Conv_seg_permuted size: \", conv_seg_permuted.size())\n","\n","        conv_seg_reshaped = conv_seg_permuted.view(conv_seg_permuted.size()[0],conv_seg_permuted.size()[1],self.dim_capsule,self.num_classes)\n","        #print(\"Conv_seg_reshaped size: \", conv_seg_reshaped.size())\n","\n","\n","        #conv_seg_reshaped_permuted = conv_seg_reshaped.permute(0,3,1,2)\n","        #print(\"Conv_seg_reshaped_permuted size: \", conv_seg_reshaped_permuted.size())\n","\n","        conv_seg_norm = torch.sigmoid(self.batch_seg(conv_seg_reshaped))\n","        #print(\"Conv_seg_norm: \", conv_seg_norm.size())\n","\n","\n","        #conv_seg_norm_permuted = conv_seg_norm.permute(0,2,3,1)\n","        #print(\"Conv_seg_norm_permuted size: \", conv_seg_norm_permuted.size())\n","\n","        output_segmentation = torch.sqrt(torch.sum(torch.square(conv_seg_norm-0.5), dim=2)*4/self.dim_capsule)\n","        #print(\"Output_segmentation size: \", output_segmentation.size())\n","\n","\n","        # ---------------\n","        # Spotting module\n","        # ---------------\n","\n","        # Concatenation of the segmentation score to the capsules\n","        output_segmentation_reverse = 1-output_segmentation\n","        #print(\"Output_segmentation_reverse size: \", output_segmentation_reverse.size())\n","\n","        output_segmentation_reverse_reshaped = output_segmentation_reverse.unsqueeze(2)\n","        #print(\"Output_segmentation_reverse_reshaped size: \", output_segmentation_reverse_reshaped.size())\n","\n","\n","        output_segmentation_reverse_reshaped_permutted = output_segmentation_reverse_reshaped.permute(0,3,1,2)\n","        #print(\"Output_segmentation_reverse_reshaped_permutted size: \", output_segmentation_reverse_reshaped_permutted.size())\n","\n","        concatenation_2 = torch.cat((conv_seg, output_segmentation_reverse_reshaped_permutted), dim=1)\n","        #print(\"Concatenation_2 size: \", concatenation_2.size())\n","\n","        conv_spot = self.max_pool_spot(F.relu(concatenation_2))\n","        #print(\"Conv_spot size: \", conv_spot.size())\n","\n","        conv_spot_1 = F.relu(self.conv_spot_1(self.pad_spot_1(conv_spot)))\n","        #print(\"Conv_spot_1 size: \", conv_spot_1.size())\n","\n","        conv_spot_1_pooled = self.max_pool_spot_1(conv_spot_1)\n","        #print(\"Conv_spot_1_pooled size: \", conv_spot_1_pooled.size())\n","\n","        conv_spot_2 = F.relu(self.conv_spot_2(self.pad_spot_2(conv_spot_1_pooled)))\n","        #print(\"Conv_spot_2 size: \", conv_spot_2.size())\n","\n","        conv_spot_2_pooled = self.max_pool_spot_2(conv_spot_2)\n","        #print(\"Conv_spot_2_pooled size: \", conv_spot_2_pooled.size())\n","\n","        spotting_reshaped = conv_spot_2_pooled.view(conv_spot_2_pooled.size()[0],-1,1,1)\n","        #print(\"Spotting_reshape size: \", spotting_reshaped.size())\n","\n","        # Confindence branch\n","        conf_pred = torch.sigmoid(self.conv_conf(spotting_reshaped).view(spotting_reshaped.shape[0],self.num_detections,2))\n","        #print(\"Conf_pred size: \", conf_pred.size())\n","\n","        # Class branch\n","        conf_class = self.softmax(self.conv_class(spotting_reshaped).view(spotting_reshaped.shape[0],self.num_detections,self.num_classes))\n","        #print(\"Conf_class size: \", conf_class.size())\n","\n","        output_spotting = torch.cat((conf_pred,conf_class),dim=-1)\n","        #print(\"Output_spotting size: \", output_spotting.size())\n","\n","\n","        return output_segmentation, output_spotting\n"],"metadata":{"id":"rHJ33LWTmo3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","#\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6HVzeu0eqUbp","executionInfo":{"status":"ok","timestamp":1733533739802,"user_tz":480,"elapsed":4026,"user":{"displayName":"Adithya Embar","userId":"03622903149941864887"}},"outputId":"9c62c420-d2c6-4c65-a120-d9379b630818"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","\n","# Path to the directory\n","directory_path = \"/content/drive/MyDrive/files/soccernet\"\n","\n","# List all files in the directory\n","if os.path.exists(directory_path):\n","    files = os.listdir(directory_path)\n","    print(f\"Files in {directory_path}:\")\n","    for file in files:\n","        print(file)\n","else:\n","    print(f\"The directory {directory_path} does not exist.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3MYNoCin-A7","executionInfo":{"status":"ok","timestamp":1733533779094,"user_tz":480,"elapsed":308,"user":{"displayName":"Adithya Embar","userId":"03622903149941864887"}},"outputId":"b4424d42-ebb9-499b-b161-fdf5adcd9ddc"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Files in /content/drive/MyDrive/files/soccernet:\n","england_epl\n","europe_uefa-champions-league\n","france_ligue-1\n","germany_bundesliga\n","model.pth.tar\n"]}]},{"cell_type":"markdown","source":["## SEB visualizations"],"metadata":{"id":"sGu4IK2pnmvV"}},{"cell_type":"code","source":["# visualizations.py\n","\n","import sys\n","import os\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","from captum.attr import Saliency\n","from scipy.ndimage import gaussian_filter1d\n","\n","# add the folder to Python's search path\n","sys.path.append(os.path.abspath(\"sn-spotting/Benchmarks/CALF/src\"))\n","\n","from dataset import SoccerNetClipsTesting\n","\n","# Initialize model\n","model = ContextAwareModelSeb(\n","    input_size=512,\n","    num_classes=17,\n","    chunk_size=240,\n","    dim_capsule=16,\n","    receptive_field=80,\n","    num_detections=15,\n","    framerate=2\n",").cuda()\n","\n","# Load the model weights\n","checkpoint = torch.load('/content/drive/MyDrive/files/soccernet/squeeze/model.pth.tar', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","model.load_state_dict(checkpoint['state_dict'])\n","model.eval()\n","\n","# Initialize dataset and dataloader\n","dataset = SoccerNetClipsTesting(\n","    path=\"/drive/MyDrive/files/soccernet\",  # Correct path\n","    features=\"ResNET_TF2_PCA512.npy\",      # Match downloaded features\n","    framerate=2,\n","    chunk_size=240,\n","    receptive_field=80\n",")\n","\n","dataloader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=1,\n","    shuffle=False,\n","    num_workers=1,\n","    pin_memory=True\n",")\n","\n","# Fetch a batch of data\n","feat_half1, feat_half2, label_half1, label_half2 = next(iter(dataloader))\n","inputs = feat_half1.cuda().squeeze(0).unsqueeze(1) # (batch_size, channels, chunk_size, num_features)\n","print(\"inputs shape:\", inputs.shape) # torch.Size([34, 1, 240, 512])\n","inputs.requires_grad = True\n","\n","# Define the target class index\n","target_class = 2  # Goal (cross-referenced with EVENT_DICTIONARY_V2)\n","\n","# Wrapper for the model's forward pass\n","def forward_wrapper(inputs):\n","    output_segmentation, _ = model(inputs)\n","    target_scores = output_segmentation[:, :, target_class]\n","    return target_scores.mean(dim=1)\n","\n","# Generate saliency maps\n","saliency = Saliency(forward_wrapper)\n","attributions = saliency.attribute(inputs)  # Gradients w.r.t. input\n","saliency_map = attributions.squeeze().cpu().detach().numpy()  # (chunks, frames, features)\n","\n","# Aggregate saliency information\n","temporal_saliency = saliency_map.mean(axis=-1)  # Average over features\n","spatial_saliency = saliency_map.mean(axis=(0, 1))  # Average over chunks and frames\n","\n","# ---- Temporal Saliency ----\n","temporal_saliency_flat = temporal_saliency.reshape(-1)\n","temporal_saliency_flat = np.maximum(temporal_saliency_flat, 0)\n","temporal_saliency_flat = (temporal_saliency_flat - temporal_saliency_flat.min()) / (\n","    temporal_saliency_flat.max() - temporal_saliency_flat.min()\n",")\n","\n","# define the target class index\n","target_class = 2 # goal, cross referenced with EVENT_DICTIONARY_V2\n","\n","# Specify the number of classes to plot\n","num_classes_to_plot = 17  # Change this value to plot a specific number of classes\n","\n","# Forward pass to get prediction confidence\n","output_segmentation, _ = model(inputs)\n","confidence_scores_per_frame = output_segmentation.squeeze(0).cpu().detach().numpy()  # Shape: (frames, classes, features)\n","print(f\"Confidence scores per frame shape: {confidence_scores_per_frame.shape}\")  # Debug: Expected (34, 240, 17)\n","\n","# Initialize the plot\n","plt.figure(figsize=(12, 6))\n","\n","# Loop through the specified number of classes and calculate their average confidence scores\n","for class_idx in range(min(num_classes_to_plot, confidence_scores_per_frame.shape[-1])):  # Limit to specified number of classes\n","    # Extract confidence scores for the current class across all chunks and frames\n","    class_confidence = confidence_scores_per_frame[..., class_idx]  # Shape: (34, 240)\n","    print(f\"Class {class_idx} confidence shape: {class_confidence.shape}\") # Ensure correct shape\n","\n","    # Calculate average confidence across chunks for each frame\n","    class_average_confidence = class_confidence.mean(axis=0)  # Shape: (240,)\n","\n","    # Normalize confidence scores for the current class\n","    normalized_class_confidence = (class_average_confidence - class_average_confidence.min()) / (\n","        class_average_confidence.max() - class_average_confidence.min()\n","    )\n","\n","    # Plot the normalized confidence scores for the current class\n","    plt.plot(range(len(normalized_class_confidence)), normalized_class_confidence, label=f\"Class {class_idx}\")\n","\n","# Add title, labels, and legend\n","plt.title(f\"SEB: Normalized Average Confidence for All Classes Over Frames\")\n","plt.xlabel(\"Frame Index\")\n","plt.ylabel(\"Normalized Confidence Score\")\n","plt.legend(loc='upper right', fontsize='small', ncol=2)  # Compact legend\n","plt.tight_layout()\n","plt.grid(alpha=0.3)\n","plt.show()\n","\n","# Hook for intermediate layer outputs\n","activation = {}\n","\n","def get_activation(name):\n","    def hook(model, input, output):\n","        activation[name] = output.detach()\n","    return hook\n","\n","model.conv_1.register_forward_hook(get_activation('conv_1'))\n","\n","# Forward pass\n","output_segmentation, _ = model(inputs)\n","feature_maps = activation['conv_1'].squeeze(0).cpu().numpy()  # Shape: (channels, frames, features)\n","\n","# Plot feature maps for a specific frame\n","specific_frame = 100  # Example frame index\n","feature_map_frame = feature_maps[:, specific_frame, :]  # Shape: (channels, features)\n","\n","plt.figure(figsize=(12, 6))\n","plt.imshow(feature_map_frame, aspect='auto', cmap='viridis')\n","plt.colorbar(label=\"Activation Value\")\n","plt.title(f\"SEB: Feature Map Activations for Frame {specific_frame}\")\n","plt.xlabel(\"Feature Index\")\n","plt.ylabel(\"Channel Index\")\n","plt.tight_layout()\n","plt.show()\n","\n","\n","# Specify the number of classes to include in the plot\n","num_classes_to_plot = 17  # Adjust this number to plot fewer or more classes (1 to 17)\n","\n","# Forward pass to get class probabilities\n","class_probabilities = torch.softmax(output_segmentation, dim=-1).squeeze(0).cpu().detach().numpy()  # Shape: (34, 240, 17)\n","print(f\"Class probabilities shape: {class_probabilities.shape}\")  # Debug: Expected (34, 240, 17)\n","\n","# Ensure the specified number of classes does not exceed the available classes\n","num_classes_to_plot = min(num_classes_to_plot, class_probabilities.shape[-1])\n","print(f\"Number of classes to plot: {num_classes_to_plot}\")\n","\n","# Initialize a plot\n","plt.figure(figsize=(12, 6))\n","\n","# Loop through the specified number of classes and plot their average probability\n","for class_idx in range(num_classes_to_plot):  # Iterate over the specified number of classes\n","    # Extract probabilities for the current class across all chunks and frames\n","    class_probabilities_current = class_probabilities[..., class_idx]  # Shape: (34, 240)\n","\n","    # Calculate the average probability for the current class across chunks\n","    class_average_probabilities = class_probabilities_current.mean(axis=0)  # Shape: (240,)\n","\n","    # Plot the average probabilities for the current class\n","    plt.plot(range(class_average_probabilities.shape[0]), class_average_probabilities, label=f\"Class {class_idx}\")\n","\n","# Add title, labels, and legend\n","plt.title(f\"SEB: Average Class Probabilities Over Frames (All Classes)\")\n","plt.xlabel(\"Frame Index\")\n","plt.ylabel(\"Average Probability\")\n","plt.legend(loc='upper right', fontsize='small', ncol=2)  # Compact legend\n","plt.grid(alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n","\n","# ---- Temporal Saliency Plot ----\n","temporal_saliency_flat = temporal_saliency.reshape(-1)\n","temporal_saliency_flat = np.maximum(temporal_saliency_flat, 0)\n","temporal_saliency_flat = (temporal_saliency_flat - temporal_saliency_flat.min()) / (\n","    temporal_saliency_flat.max() - temporal_saliency_flat.min()\n",")\n","\n","# Average activations over the receptive field\n","receptive_field_size = model.receptive_field\n","average_activations = [\n","    temporal_saliency[max(0, i-receptive_field_size//2):min(len(temporal_saliency), i+receptive_field_size//2)].mean()\n","    for i in range(len(temporal_saliency))\n","]\n","\n","# --- Receptive Field Activations ---\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(len(average_activations)), average_activations, label=\"Average Activation\", color='green')\n","# Highlight max and min activations for better insights\n","max_activation_idx = np.argmax(average_activations)\n","min_activation_idx = np.argmin(average_activations)\n","plt.scatter([max_activation_idx], [average_activations[max_activation_idx]], color='red', label=\"Max Activation\")\n","plt.scatter([min_activation_idx], [average_activations[min_activation_idx]], color='blue', label=\"Min Activation\")\n","plt.title(\"SEB: Average Activations Within Receptive Field\")\n","plt.xlabel(\"Frame Index\")\n","plt.ylabel(\"Average Activation\")\n","plt.legend(loc='upper right', fontsize='small')  # Simplify legend\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":438},"id":"ND1XBK51nePC","executionInfo":{"status":"error","timestamp":1733533072325,"user_tz":480,"elapsed":309,"user":{"displayName":"Adithya Embar","userId":"03622903149941864887"}},"outputId":"9d0359bf-05e2-4d2e-cdf3-f8fe8061bf1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-17-da4a45c383de>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load('/content/drive/MyDrive/files/soccernet/squeeze/model.pth.tar', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/files/soccernet/squeeze/model.pth.tar'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-da4a45c383de>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Load the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/files/soccernet/squeeze/model.pth.tar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/files/soccernet/squeeze/model.pth.tar'"]}]}]}